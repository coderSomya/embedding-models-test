{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab783929-e236-4fba-ba01-872d248535fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF...\n",
      "Created 127 chunks\n",
      "\n",
      "Testing sentence-bert-small...\n",
      "Encoding chunks...\n",
      "\n",
      "Testing sentence-bert-large...\n",
      "Encoding chunks...\n",
      "\n",
      "Testing e5-small...\n",
      "Encoding chunks...\n",
      "\n",
      "Testing e5-base...\n",
      "Encoding chunks...\n",
      "\n",
      "Testing bge-small...\n",
      "Encoding chunks...\n",
      "\n",
      "Testing bge-base...\n",
      "Encoding chunks...\n",
      "\n",
      "====================================================================================================\n",
      "MODEL PERFORMANCE COMPARISON\n",
      "====================================================================================================\n",
      "\n",
      "MODEL STATISTICS:\n",
      "              model  load_time  avg_search_time  avg_relevance  embedding_dim\n",
      "sentence-bert-small      3.910            0.044          0.331            384\n",
      "sentence-bert-large      4.791            0.049          0.363            768\n",
      "           e5-small      4.298            0.031          0.832            384\n",
      "            e5-base      4.784            0.032          0.793            768\n",
      "          bge-small      4.235            0.031          0.649            384\n",
      "           bge-base      5.198            0.030          0.647            768\n",
      "\n",
      "====================================================================================================\n",
      "DETAILED RESULTS BY QUESTION\n",
      "====================================================================================================\n",
      "\n",
      "QUESTION: Who are the authors of this paper?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "SENTENCE-BERT-SMALL (Score: 0.313):\n",
      "  1. [0.356] Yujia He\n",
      "Yunfan Xiong\n",
      "Yuxiang Luo\n",
      "Yuxiang You\n",
      "Yuxuan Liu\n",
      "Yuyang Zhou\n",
      "Y.X. Zhu\n",
      "Yanping Huang\n",
      "Yaohui L...\n",
      "  2. [0.304] ributions and Acknowledgments\n",
      "Core Contributors\n",
      "Daya Guo\n",
      "Dejian Yang\n",
      "Haowei Zhang\n",
      "Junxiao Song\n",
      "Ruoyu...\n",
      "  3. [0.279] eng Zhang\n",
      "Qiancheng Wang\n",
      "Qinyu Chen\n",
      "Qiushi Du\n",
      "Ruiqi Ge*\n",
      "Ruisong Zhang\n",
      "Ruizhe Pan\n",
      "Runji Wang\n",
      "R.J. Che...\n",
      "\n",
      "SENTENCE-BERT-LARGE (Score: 0.409):\n",
      "  1. [0.499] Yujia He\n",
      "Yunfan Xiong\n",
      "Yuxiang Luo\n",
      "Yuxiang You\n",
      "Yuxuan Liu\n",
      "Yuyang Zhou\n",
      "Y.X. Zhu\n",
      "Yanping Huang\n",
      "Yaohui L...\n",
      "  2. [0.416] Chen\n",
      "Guowei Li\n",
      "H. Zhang\n",
      "Hanwei Xu\n",
      "Honghui Ding\n",
      "Huazuo Gao\n",
      "Hui QuHui Li\n",
      "Jianzhong Guo\n",
      "Jiashi Li\n",
      "Jingc...\n",
      "  3. [0.312] Cheng\n",
      "Xin Liu\n",
      "Xin Xie\n",
      "Xingchao Liu\n",
      "Xinyu Yang\n",
      "Xinyuan Li\n",
      "Xuecheng Su\n",
      "Xuheng Lin\n",
      "X.Q. Li\n",
      "Xiangyue Jin...\n",
      "\n",
      "E5-SMALL (Score: 0.839):\n",
      "  1. [0.843] ributions and Acknowledgments\n",
      "Core Contributors\n",
      "Daya Guo\n",
      "Dejian Yang\n",
      "Haowei Zhang\n",
      "Junxiao Song\n",
      "Ruoyu...\n",
      "  2. [0.843] Yujia He\n",
      "Yunfan Xiong\n",
      "Yuxiang Luo\n",
      "Yuxiang You\n",
      "Yuxuan Liu\n",
      "Yuyang Zhou\n",
      "Y.X. Zhu\n",
      "Yanping Huang\n",
      "Yaohui L...\n",
      "  3. [0.832] the boundaries of the unknown, 2024a. URL https://qwenlm\n",
      ".github.io/blog/qwq-32b-preview/ .\n",
      "Qwen. Qw...\n",
      "\n",
      "E5-BASE (Score: 0.790):\n",
      "  1. [0.802] ributions and Acknowledgments\n",
      "Core Contributors\n",
      "Daya Guo\n",
      "Dejian Yang\n",
      "Haowei Zhang\n",
      "Junxiao Song\n",
      "Ruoyu...\n",
      "  2. [0.786] Yujia He\n",
      "Yunfan Xiong\n",
      "Yuxiang Luo\n",
      "Yuxiang You\n",
      "Yuxuan Liu\n",
      "Yuyang Zhou\n",
      "Y.X. Zhu\n",
      "Yanping Huang\n",
      "Yaohui L...\n",
      "  3. [0.782] rint arXiv:2404.04475, 2024.\n",
      "X. Feng, Z. Wan, M. Wen, S. M. McAleer, Y. Wen, W. Zhang, and J. Wang. ...\n",
      "\n",
      "BGE-SMALL (Score: 0.649):\n",
      "  1. [0.702] ributions and Acknowledgments\n",
      "Core Contributors\n",
      "Daya Guo\n",
      "Dejian Yang\n",
      "Haowei Zhang\n",
      "Junxiao Song\n",
      "Ruoyu...\n",
      "  2. [0.636] Yujia He\n",
      "Yunfan Xiong\n",
      "Yuxiang Luo\n",
      "Yuxiang You\n",
      "Yuxuan Liu\n",
      "Yuyang Zhou\n",
      "Y.X. Zhu\n",
      "Yanping Huang\n",
      "Yaohui L...\n",
      "  3. [0.608] U (Li et al.,\n",
      "2023), IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein e...\n",
      "\n",
      "BGE-BASE (Score: 0.692):\n",
      "  1. [0.715] ributions and Acknowledgments\n",
      "Core Contributors\n",
      "Daya Guo\n",
      "Dejian Yang\n",
      "Haowei Zhang\n",
      "Junxiao Song\n",
      "Ruoyu...\n",
      "  2. [0.684] Yujia He\n",
      "Yunfan Xiong\n",
      "Yuxiang Luo\n",
      "Yuxiang You\n",
      "Yuxuan Liu\n",
      "Yuyang Zhou\n",
      "Y.X. Zhu\n",
      "Yanping Huang\n",
      "Yaohui L...\n",
      "  3. [0.676] the boundaries of the unknown, 2024a. URL https://qwenlm\n",
      ".github.io/blog/qwq-32b-preview/ .\n",
      "Qwen. Qw...\n",
      "\n",
      "QUESTION: What is reasoning in case of an LLM\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "SENTENCE-BERT-SMALL (Score: 0.460):\n",
      "  1. [0.504] 2.\n",
      "P . Wang, L. Li, Z. Shao, R. Xu, D. Dai, Y. Li, D. Chen, Y. Wu, and Z. Sui. Math-shepherd: A labe...\n",
      "  2. [0.438] cation, reflection, and generating\n",
      "long CoTs, marking a significant milestone for the research commu...\n",
      "  3. [0.437] et al., 2024; Wang et al., 2023). However, these works\n",
      "heavily depended on supervised data, which ar...\n",
      "\n",
      "SENTENCE-BERT-LARGE (Score: 0.471):\n",
      "  1. [0.490] DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\n",
      "Reinforcement Learning\n",
      "DeepSeek-AI\n",
      "resea...\n",
      "  2. [0.488] cation, reflection, and generating\n",
      "long CoTs, marking a significant milestone for the research commu...\n",
      "  3. [0.437] et al., 2024; Wang et al., 2023). However, these works\n",
      "heavily depended on supervised data, which ar...\n",
      "\n",
      "E5-SMALL (Score: 0.861):\n",
      "  1. [0.865] et al., 2024; Wang et al., 2023). However, these works\n",
      "heavily depended on supervised data, which ar...\n",
      "  2. [0.864] al of LLMs to develop\n",
      "reasoning capabilities without any supervised data, focusing on their self-evo...\n",
      "  3. [0.853] 2.\n",
      "P . Wang, L. Li, Z. Shao, R. Xu, D. Dai, Y. Li, D. Chen, Y. Wu, and Z. Sui. Math-shepherd: A labe...\n",
      "\n",
      "E5-BASE (Score: 0.835):\n",
      "  1. [0.838] et al., 2024; Wang et al., 2023). However, these works\n",
      "heavily depended on supervised data, which ar...\n",
      "  2. [0.836] al of LLMs to develop\n",
      "reasoning capabilities without any supervised data, focusing on their self-evo...\n",
      "  3. [0.830] DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\n",
      "Reinforcement Learning\n",
      "DeepSeek-AI\n",
      "resea...\n",
      "\n",
      "BGE-SMALL (Score: 0.713):\n",
      "  1. [0.731] 2.\n",
      "P . Wang, L. Li, Z. Shao, R. Xu, D. Dai, Y. Li, D. Chen, Y. Wu, and Z. Sui. Math-shepherd: A labe...\n",
      "  2. [0.706] et al., 2024; Wang et al., 2023). However, these works\n",
      "heavily depended on supervised data, which ar...\n",
      "  3. [0.700] cation, reflection, and generating\n",
      "long CoTs, marking a significant milestone for the research commu...\n",
      "\n",
      "BGE-BASE (Score: 0.663):\n",
      "  1. [0.685] et al., 2024; Wang et al., 2023). However, these works\n",
      "heavily depended on supervised data, which ar...\n",
      "  2. [0.655] rcement learning (Kumar et al., 2024),\n",
      "and search algorithms such as Monte Carlo Tree Search and Bea...\n",
      "  3. [0.650] cation, reflection, and generating\n",
      "long CoTs, marking a significant milestone for the research commu...\n",
      "\n",
      "QUESTION: What is the abstract?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "SENTENCE-BERT-SMALL (Score: 0.240):\n",
      "  1. [0.246] ach group:\n",
      "𝐴𝑖=𝑟𝑖−m𝑒𝑎𝑛({𝑟1,𝑟2,···,𝑟𝐺})\n",
      "s𝑡𝑑({𝑟1,𝑟2,···,𝑟𝐺}). (3)\n",
      "5A conversation between User and Assi...\n",
      "  2. [0.237] ithin the model. DeepSeek-R1-Zero naturally acquires the\n",
      "ability to solve increasingly complex reaso...\n",
      "  3. [0.236] . Let’s verify step by step. arXiv preprint arXiv:2305.20050, 2023.\n",
      "B. Y. Lin. ZeroEval: A Unified F...\n",
      "\n",
      "SENTENCE-BERT-LARGE (Score: 0.302):\n",
      "  1. [0.354] 16References\n",
      "AI@Meta. Llama 3.1 model card, 2024. URL https://github.com/meta-llama/llama-m\n",
      "odels/bl...\n",
      "  2. [0.314] ach group:\n",
      "𝐴𝑖=𝑟𝑖−m𝑒𝑎𝑛({𝑟1,𝑟2,···,𝑟𝐺})\n",
      "s𝑡𝑑({𝑟1,𝑟2,···,𝑟𝐺}). (3)\n",
      "5A conversation between User and Assi...\n",
      "  3. [0.237] the boundaries of the unknown, 2024a. URL https://qwenlm\n",
      ".github.io/blog/qwq-32b-preview/ .\n",
      "Qwen. Qw...\n",
      "\n",
      "E5-SMALL (Score: 0.797):\n",
      "  1. [0.805] fulness, we focus exclusively on the final summary, ensuring that the\n",
      "assessment emphasizes the util...\n",
      "  2. [0.795] . URL https://openai.com/index/hello-gpt-4o/ .\n",
      "OpenAI. Learning to reason with llms, 2024b. URL http...\n",
      "  3. [0.792] 16References\n",
      "AI@Meta. Llama 3.1 model card, 2024. URL https://github.com/meta-llama/llama-m\n",
      "odels/bl...\n",
      "\n",
      "E5-BASE (Score: 0.771):\n",
      "  1. [0.777] ntrast, when creating cold-start data for DeepSeek-R1,\n",
      "we design a readable pattern that includes a ...\n",
      "  2. [0.769] k (Metric)Claude-3.5- GPT-4o DeepSeek OpenAI OpenAI DeepSeek\n",
      "Sonnet-1022 0513 V3 o1-mini o1-1217 R1\n",
      "...\n",
      "  3. [0.767] mpleqa: A chinese factuality evaluation for large language models. arXiv preprint\n",
      "arXiv:2411.07140, ...\n",
      "\n",
      "BGE-SMALL (Score: 0.595):\n",
      "  1. [0.616] ach group:\n",
      "𝐴𝑖=𝑟𝑖−m𝑒𝑎𝑛({𝑟1,𝑟2,···,𝑟𝐺})\n",
      "s𝑡𝑑({𝑟1,𝑟2,···,𝑟𝐺}). (3)\n",
      "5A conversation between User and Assi...\n",
      "  2. [0.586] fulness, we focus exclusively on the final summary, ensuring that the\n",
      "assessment emphasizes the util...\n",
      "  3. [0.584] . Let’s verify step by step. arXiv preprint arXiv:2305.20050, 2023.\n",
      "B. Y. Lin. ZeroEval: A Unified F...\n",
      "\n",
      "BGE-BASE (Score: 0.596):\n",
      "  1. [0.604] ach group:\n",
      "𝐴𝑖=𝑟𝑖−m𝑒𝑎𝑛({𝑟1,𝑟2,···,𝑟𝐺})\n",
      "s𝑡𝑑({𝑟1,𝑟2,···,𝑟𝐺}). (3)\n",
      "5A conversation between User and Assi...\n",
      "  2. [0.597] . Let’s verify step by step. arXiv preprint arXiv:2305.20050, 2023.\n",
      "B. Y. Lin. ZeroEval: A Unified F...\n",
      "  3. [0.588] ithin the model. DeepSeek-R1-Zero naturally acquires the\n",
      "ability to solve increasingly complex reaso...\n",
      "\n",
      "QUESTION: What are the conclusions?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "SENTENCE-BERT-SMALL (Score: 0.310):\n",
      "  1. [0.359] X. Du, M. R. G. Madani, C. Barale, R. McHardy, J. Harris, J. Kaddour, E. van Krieken, and\n",
      "P . Minerv...\n",
      "  2. [0.321] fulness, we focus exclusively on the final summary, ensuring that the\n",
      "assessment emphasizes the util...\n",
      "  3. [0.249] the boundaries of the unknown, 2024a. URL https://qwenlm\n",
      ".github.io/blog/qwq-32b-preview/ .\n",
      "Qwen. Qw...\n",
      "\n",
      "SENTENCE-BERT-LARGE (Score: 0.271):\n",
      "  1. [0.290] 16References\n",
      "AI@Meta. Llama 3.1 model card, 2024. URL https://github.com/meta-llama/llama-m\n",
      "odels/bl...\n",
      "  2. [0.277] ht incentives, and it autonomously develops advanced problem-solving strategies. The\n",
      "“aha moment” se...\n",
      "  3. [0.248] the boundaries of the unknown, 2024a. URL https://qwenlm\n",
      ".github.io/blog/qwq-32b-preview/ .\n",
      "Qwen. Qw...\n",
      "\n",
      "E5-SMALL (Score: 0.830):\n",
      "  1. [0.835] fulness, we focus exclusively on the final summary, ensuring that the\n",
      "assessment emphasizes the util...\n",
      "  2. [0.830] the boundaries of the unknown, 2024a. URL https://qwenlm\n",
      ".github.io/blog/qwq-32b-preview/ .\n",
      "Qwen. Qw...\n",
      "  3. [0.826] . URL https://openai.com/index/hello-gpt-4o/ .\n",
      "OpenAI. Learning to reason with llms, 2024b. URL http...\n",
      "\n",
      "E5-BASE (Score: 0.777):\n",
      "  1. [0.784] fulness, we focus exclusively on the final summary, ensuring that the\n",
      "assessment emphasizes the util...\n",
      "  2. [0.777] ntrast, when creating cold-start data for DeepSeek-R1,\n",
      "we design a readable pattern that includes a ...\n",
      "  3. [0.770] l results, shown in Table 6, demonstrate that the 32B base model, after large-scale\n",
      "14ModelAIME 2024...\n",
      "\n",
      "BGE-SMALL (Score: 0.641):\n",
      "  1. [0.669] fulness, we focus exclusively on the final summary, ensuring that the\n",
      "assessment emphasizes the util...\n",
      "  2. [0.638] X. Du, M. R. G. Madani, C. Barale, R. McHardy, J. Harris, J. Kaddour, E. van Krieken, and\n",
      "P . Minerv...\n",
      "  3. [0.615] ntrast, when creating cold-start data for DeepSeek-R1,\n",
      "we design a readable pattern that includes a ...\n",
      "\n",
      "BGE-BASE (Score: 0.638):\n",
      "  1. [0.685] fulness, we focus exclusively on the final summary, ensuring that the\n",
      "assessment emphasizes the util...\n",
      "  2. [0.623] ach group:\n",
      "𝐴𝑖=𝑟𝑖−m𝑒𝑎𝑛({𝑟1,𝑟2,···,𝑟𝐺})\n",
      "s𝑡𝑑({𝑟1,𝑟2,···,𝑟𝐺}). (3)\n",
      "5A conversation between User and Assi...\n",
      "  3. [0.605] tokens for the models.\n",
      "We found that using greedy decoding to evaluate long-output reasoning models ...\n",
      "\n",
      "====================================================================================================\n",
      "RECOMMENDATIONS\n",
      "====================================================================================================\n",
      "Best Relevance: e5-small (Score: 0.832)\n",
      "Fastest Search: bge-base (29.9ms)\n",
      "Fastest Load: sentence-bert-small (3.9s)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import PyPDF2\n",
    "import fitz\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "PDF_PATH = \"deepseek_R1.pdf\"\n",
    "CHUNK_SIZE = 500\n",
    "OVERLAP = 50\n",
    "TOP_K = 3\n",
    "\n",
    "MODELS = {\n",
    "    'sentence-bert-small': 'all-MiniLM-L6-v2',\n",
    "    'sentence-bert-large': 'all-mpnet-base-v2',\n",
    "    'e5-small': 'intfloat/e5-small-v2',\n",
    "    'e5-base': 'intfloat/e5-base-v2',\n",
    "    'bge-small': 'BAAI/bge-small-en-v1.5',\n",
    "    'bge-base': 'BAAI/bge-base-en-v1.5',\n",
    "}\n",
    "\n",
    "TEST_QUESTIONS = [\n",
    "    \"Who are the authors of this paper?\",\n",
    "    \"What is reasoning in case of an LLM\",\n",
    "    \"What is the abstract?\",\n",
    "    \"What are the conclusions?\",\n",
    "]\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            for page in doc:\n",
    "                text += page.get_text()\n",
    "    except:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def chunk_text(text, chunk_size=500, overlap=50):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        if end < len(text):\n",
    "            last_period = chunk.rfind('.')\n",
    "            if last_period > start + chunk_size // 2:\n",
    "                end = start + last_period + 1\n",
    "                chunk = text[start:end]\n",
    "        chunks.append(chunk.strip())\n",
    "        start = end - overlap\n",
    "        if start >= len(text):\n",
    "            break\n",
    "    return chunks\n",
    "\n",
    "class EmbeddingModel:\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self.load_time = 0\n",
    "        \n",
    "    def load_model(self):\n",
    "        start_time = time.time()\n",
    "        if any(x in self.model_name for x in ['all-MiniLM', 'all-mpnet', 'e5-', 'bge-']):\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "        else:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "            self.model = AutoModel.from_pretrained(self.model_name)\n",
    "        self.load_time = time.time() - start_time\n",
    "        \n",
    "    def encode(self, texts):\n",
    "        if isinstance(self.model, SentenceTransformer):\n",
    "            return self.model.encode(texts)\n",
    "        else:\n",
    "            inputs = self.tokenizer(texts, padding=True, truncation=True, \n",
    "                                  return_tensors='pt', max_length=512)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            return outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "def search_chunks(query_embedding, chunk_embeddings, chunks, top_k=3):\n",
    "    similarities = cosine_similarity([query_embedding], chunk_embeddings)[0]\n",
    "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "    \n",
    "    results = []\n",
    "    for i, idx in enumerate(top_indices):\n",
    "        results.append({\n",
    "            'rank': i + 1,\n",
    "            'chunk': chunks[idx],\n",
    "            'score': similarities[idx],\n",
    "            'chunk_id': idx\n",
    "        })\n",
    "    return results\n",
    "\n",
    "def run_comparison():\n",
    "    print(\"Loading PDF...\")\n",
    "    text = extract_text_from_pdf(PDF_PATH)\n",
    "    chunks = chunk_text(text, CHUNK_SIZE, OVERLAP)\n",
    "    print(f\"Created {len(chunks)} chunks\")\n",
    "    \n",
    "    all_results = {}\n",
    "    model_stats = []\n",
    "    \n",
    "    for model_key, model_name in MODELS.items():\n",
    "        print(f\"\\nTesting {model_key}...\")\n",
    "        \n",
    "        try:\n",
    "            model = EmbeddingModel(model_name)\n",
    "            model.load_model()\n",
    "            \n",
    "            print(f\"Encoding chunks...\")\n",
    "            chunk_embeddings = model.encode(chunks)\n",
    "            \n",
    "            model_results = []\n",
    "            total_search_time = 0\n",
    "            \n",
    "            for question in TEST_QUESTIONS:\n",
    "                start_time = time.time()\n",
    "                query_embedding = model.encode([question])[0]\n",
    "                search_results = search_chunks(query_embedding, chunk_embeddings, chunks, TOP_K)\n",
    "                search_time = time.time() - start_time\n",
    "                total_search_time += search_time\n",
    "                \n",
    "                model_results.append({\n",
    "                    'question': question,\n",
    "                    'results': search_results,\n",
    "                    'avg_score': np.mean([r['score'] for r in search_results])\n",
    "                })\n",
    "            \n",
    "            all_results[model_key] = model_results\n",
    "            \n",
    "            model_stats.append({\n",
    "                'model': model_key,\n",
    "                'load_time': model.load_time,\n",
    "                'avg_search_time': total_search_time / len(TEST_QUESTIONS),\n",
    "                'avg_relevance': np.mean([r['avg_score'] for r in model_results]),\n",
    "                'embedding_dim': chunk_embeddings.shape[1]\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return all_results, pd.DataFrame(model_stats)\n",
    "\n",
    "def print_results(all_results, stats_df):\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    print(\"\\nMODEL STATISTICS:\")\n",
    "    print(stats_df.round(3).to_string(index=False))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"DETAILED RESULTS BY QUESTION\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    for question in TEST_QUESTIONS:\n",
    "        print(f\"\\nQUESTION: {question}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for model_key in all_results.keys():\n",
    "            model_results = all_results[model_key]\n",
    "            question_result = next((r for r in model_results if r['question'] == question), None)\n",
    "            \n",
    "            if question_result:\n",
    "                print(f\"\\n{model_key.upper()} (Score: {question_result['avg_score']:.3f}):\")\n",
    "                for result in question_result['results']:\n",
    "                    chunk_preview = result['chunk'][:100] + \"...\" if len(result['chunk']) > 100 else result['chunk']\n",
    "                    print(f\"  {result['rank']}. [{result['score']:.3f}] {chunk_preview}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"RECOMMENDATIONS\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    best_relevance = stats_df.loc[stats_df['avg_relevance'].idxmax()]\n",
    "    fastest_search = stats_df.loc[stats_df['avg_search_time'].idxmin()]\n",
    "    fastest_load = stats_df.loc[stats_df['load_time'].idxmin()]\n",
    "    \n",
    "    print(f\"Best Relevance: {best_relevance['model']} (Score: {best_relevance['avg_relevance']:.3f})\")\n",
    "    print(f\"Fastest Search: {fastest_search['model']} ({fastest_search['avg_search_time']*1000:.1f}ms)\")\n",
    "    print(f\"Fastest Load: {fastest_load['model']} ({fastest_load['load_time']:.1f}s)\")\n",
    "\n",
    "\n",
    "results, stats = run_comparison()\n",
    "print_results(results, stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26f363e3-7924-4011-8f29-17f9270c5bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement faiss (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for faiss\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c90440e-be80-4e70-a856-11dc7e240093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9895d6db-942c-49e4-a905-0a4ba765d900",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
